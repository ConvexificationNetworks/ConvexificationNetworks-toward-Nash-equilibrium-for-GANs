{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# これを300 epochほどやろう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: UTF-8\n",
    "# これは成功！（100 epochで十分）\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "#from PIL import Image\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import initializers\n",
    "from chainer import training\n",
    "from chainer import Variable\n",
    "from chainer import cuda\n",
    "from chainer.dataset import iterator as iterator_module\n",
    "from chainer.training import extensions\n",
    "from chainer.dataset import convert\n",
    "from chainer import Variable\n",
    "#from evaluate import out_generated_image\n",
    "from chainer import variable\n",
    "from data import GmmDataset\n",
    "from utils import out_generated, calcEMD\n",
    "\n",
    "# np = cuda.cupy\n",
    "#random_state = np.random.RandomState(123)\n",
    "\n",
    "\n",
    "# Updater\n",
    "\n",
    "class WGANUpdater(training.StandardUpdater):\n",
    "    def __init__(self, iterator, generator, critic,\n",
    "                 n_c, opt_g, opt_c, lam, lam2, n_hidden, device):\n",
    "        if isinstance(iterator, iterator_module.Iterator):\n",
    "            iterator = {'main': iterator}\n",
    "        self._iterators = iterator\n",
    "        self.generator = generator\n",
    "        self.critic = critic\n",
    "        self.n_c = n_c\n",
    "        self._optimizers = {'generator': opt_g, 'critic': opt_c}\n",
    "        self.lam = lam\n",
    "        self.lam2 = lam2\n",
    "        self.device = device\n",
    "        self.converter = convert.concat_examples\n",
    "        self.iteration = 0\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "    def update_core(self):\n",
    "        batch = self.get_iterator('main').next()\n",
    "        batchsize = len(batch)\n",
    "\n",
    "        # Step1 Generate\n",
    "        z = Variable(np.asarray(self.generator.make_hidden(batchsize)))\n",
    "        #print(z)\n",
    "        x_gen = self.generator(z)\n",
    "        #print(\"iteration is \", self.iteration)\n",
    "        #print(\"x_gen is \", x_gen)\n",
    "        y_gen = self.critic(x_gen)\n",
    "\n",
    "        # Step2 real\n",
    "        x_real = Variable(np.array(batch))\n",
    "        y_real = self.critic(x_real)\n",
    "\n",
    "        # Step3 Compute loss for wgan_gp\n",
    "        eps = np.random.uniform(0, 1, (batchsize, 1)).astype(\"f\")\n",
    "        x_mid = eps * x_real + (1.0 - eps) * x_gen\n",
    "        x_mid_v = Variable(x_mid.data)\n",
    "        y_mid = self.critic(x_mid_v)\n",
    "        dydx = chainer.grad([y_mid], [x_mid_v], enable_double_backprop=True)[0]\n",
    "        dydx = F.sqrt(F.sum(F.square(dydx), axis=1))\n",
    "        loss_gp = self.lam * F.mean_squared_error(dydx, np.ones_like(dydx.data))\n",
    "        loss_cri = F.sum(-y_real) / batchsize\n",
    "        #print(loss_cri)\n",
    "        loss_cri += F.sum(y_gen) / batchsize\n",
    "        \n",
    "        loss_sp = self.lam2 * F.absolute(F.sum(self.critic.inter.W) - 1)\n",
    "        loss_all = loss_cri + loss_gp + loss_sp\n",
    "        \n",
    "        # Step4 Update critic\n",
    "        self.critic.cleargrads()\n",
    "        loss_all.backward()\n",
    "        self._optimizers['critic'].update()\n",
    "\n",
    "        # Step5 Update generator\n",
    "        \n",
    "        if self.iteration % self.n_c == 0:\n",
    "            loss_gen = F.sum(-y_gen) / batchsize\n",
    "            #print(loss_gen)\n",
    "            self.generator.cleargrads()\n",
    "            loss_gen.backward()\n",
    "            self._optimizers['generator'].update()\n",
    "            chainer.reporter.report({'loss/generator': loss_gen})\n",
    "\n",
    "        \"\"\"\n",
    "        if self.iteration < 2500 and self.iteration % 100 == 0:\n",
    "            loss_gen = F.sum(-y_gen) / batchsize\n",
    "            loss_sp = self.lam2 * F.absolute(F.sum(self.generator.inter.W) - 1)\n",
    "            loss_gen += loss_sp\n",
    "            self.generator.cleargrads()\n",
    "            loss_gen.backward()\n",
    "            self._optimizers['generator'].update()\n",
    "            chainer.reporter.report({'loss/generator': loss_gen})\n",
    "\n",
    "        if self.iteration > 2500 and self.iteration % self.n_c == 0:\n",
    "            loss_gen = F.sum(-y_gen) / batchsize\n",
    "            loss_sp = self.lam2 * F.absolute(F.sum(self.generator.inter.W) - 1)\n",
    "            loss_gen += loss_sp\n",
    "            self.generator.cleargrads()\n",
    "            loss_gen.backward()\n",
    "            self._optimizers['generator'].update()\n",
    "            chainer.reporter.report({'loss/generator': loss_gen})\n",
    "        \"\"\"\n",
    "\n",
    "        # Step6 Report\n",
    "        chainer.reporter.report({'loss/critic': loss_cri})\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='WGAN')\n",
    "    parser.add_argument('--batchsize', '-b', type=int, default=64,\n",
    "                        help='Number of images in each mini-batch')\n",
    "    parser.add_argument('--datasize', '-d', type=int, default=10000,\n",
    "                        help='Number of samples')\n",
    "    parser.add_argument('--lam', '-l', type=int, default=0.5,\n",
    "                        help='Hyperparameter of gp')\n",
    "    parser.add_argument('--lam2', '-l2', type=int, default=1.0,\n",
    "                        help='Hyperparameter of gp')\n",
    "    parser.add_argument('--n_hidden', type=int, default=100,\n",
    "                        help='latent variable')\n",
    "    parser.add_argument('--seed', '-s', type=int, default=111,\n",
    "                        help='seed number')\n",
    "    parser.add_argument('--epoch', '-e', type=int, default=300,\n",
    "                        help='Number of sweeps over the dataset to train')\n",
    "    parser.add_argument('--gpu', '-g', type=int, default=-1,\n",
    "                        help='GPU ID (negative value indicates CPU)')\n",
    "    parser.add_argument('--out', '-o', default='result',\n",
    "                        help='Directory to output the result')\n",
    "    parser.add_argument('--resume', '-r', default='',\n",
    "                        help='Resume the training from snapshot')\n",
    "    parser.add_argument('--unit', '-u', type=int, default=1000,\n",
    "                        help='Number of units')\n",
    "    parser.add_argument('--setting', '-set', type=int, default=10,\n",
    "                        help='Number of units')\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    # parameter set\n",
    "    ## Optimizers\n",
    "    alpha = 0.002\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "\n",
    "    ## Network\n",
    "    dim = 2\n",
    "    num_nets = 1\n",
    "    wscale = 0.02\n",
    "\n",
    "    # Networks\n",
    "    generator = Generator(dim, num_nets, args.n_hidden, wscale)\n",
    "    critic = Critic(num_nets, wscale)\n",
    "\n",
    "    if args.gpu >= 0:\n",
    "        chainer.cuda.get_device(args.gpu).use()\n",
    "        generator.to_gpu()\n",
    "        critic.to_gpu()\n",
    "\n",
    "    # Optimizer set\n",
    "    opt_g = chainer.optimizers.Adam(alpha = alpha, beta1 = beta1, beta2 = beta2, weight_decay_rate=0.0001)\n",
    "    #opt_g = chainer.optimizers.RMSprop(lr=0.00005)\n",
    "    opt_g.setup(generator)\n",
    "    #opt_g.add_hook(chainer.optimizer.WeightDecay(0.00001), 'hook_dec')\n",
    "    opt_g.add_hook(chainer.optimizer.GradientClipping(1))\n",
    "\n",
    "    opt_c = chainer.optimizers.Adam(alpha = alpha, beta1 = beta1, beta2 = beta2, weight_decay_rate=0.0001)\n",
    "    #opt_c = chainer.optimizers.RMSprop(lr=0.00005)\n",
    "    opt_c.setup(critic)\n",
    "    #opt_c.add_hook(chainer.optimizer.WeightDecay(0.00001), 'hook_dec')\n",
    "    opt_c.add_hook(chainer.optimizer.GradientClipping(1))\n",
    "\n",
    "    # Dataset\n",
    "    \"\"\"\n",
    "    train, test = chainer.datasets.get_mnist(withlabel=False, ndim=3, scale=255.)\n",
    "    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = GmmDataset(args.datasize, args.seed, std=0.02, scale=2)\n",
    "    train_iter = chainer.iterators.SerialIterator(dataset, args.batchsize)\n",
    "\n",
    "    # Trainer\n",
    "    updater = WGANUpdater(train_iter, generator, critic, 5, opt_g, opt_c, args.lam, args.lam2, args.n_hidden, device=args.gpu)\n",
    "    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)\n",
    "\n",
    "    # Extensions\n",
    "    # trainer.extend(extensions.dump_graph('wasserstein distance'))\n",
    "    \"\"\"\n",
    "    epoch_interval = (50, 'epoch')\n",
    "    trainer.extend(extensions.snapshot(filename='2dim2snap_epoch_{.updater.epoch}.npz'), trigger=epoch_interval)\n",
    "    trainer.extend(extensions.snapshot_object(generator, '2dim2gensnap_epoch_{.updater.epoch}.npz'), trigger=epoch_interval)\n",
    "    trainer.extend(extensions.snapshot_object(critic, '2dim2dissnap_epoch_{.updater.epoch}.npz'), trigger=epoch_interval)\n",
    "    \"\"\"\n",
    "    \n",
    "    trainer.extend(extensions.LogReport())\n",
    "\n",
    "    trainer.extend(\n",
    "        extensions.PlotReport(['loss/critic'],\n",
    "                              'epoch', file_name='criticloss_2dim%d_3.png' % args.setting))\n",
    "\n",
    "    trainer.extend(\n",
    "        extensions.PlotReport(\n",
    "            ['loss/generator'], 'epoch', file_name='genloss_2dim%d_3.png' % args.setting))\n",
    "\n",
    "    trainer.extend(extensions.PrintReport(\n",
    "        ['epoch', 'loss/critic', 'loss/generator', 'elapsed_time']))\n",
    "\n",
    "    out = \"/Users/keiikegami/Dropbox/research/Imaizumi-Sensei/paper_result/2dim/result%d_3\" % args.setting\n",
    "\n",
    "    trainer.extend(calcEMD(generator), trigger=(1, 'epoch'))\n",
    "    trainer.extend(extensions.LogReport(log_name = \"log_2dim%d_3\" % args.setting))\n",
    "    trainer.extend(out_generated(generator, args.seed + 10, out, radius=2), trigger=(10, 'epoch'))\n",
    "    #trainer.extend(extensions.ProgressBar())\n",
    "\n",
    "    if args.resume:\n",
    "        chainer.serializers.load_npz(args.resume, trainer)\n",
    "\n",
    "    # Run\n",
    "    trainer.run()\n",
    "\n",
    "\n",
    "# orriginal class of link\n",
    "class Intersection2(chainer.Link):\n",
    "\n",
    "    def __init__(self, outdim, numnet):\n",
    "        super(Intersection2, self).__init__()\n",
    "        self.outdim = outdim\n",
    "        self.numnet = numnet\n",
    "        with self.init_scope():\n",
    "            W = chainer.initializers.One()\n",
    "            self.W = variable.Parameter(W)\n",
    "            self.W.initialize((self.numnet, 1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.outdim == 1:\n",
    "            weight = F.relu(self.W.T)\n",
    "        else:\n",
    "            weight = F.relu(self.make_weight(self.W))\n",
    "\n",
    "        return F.matmul(weight, x)\n",
    "\n",
    "    def make_weight(self, array):\n",
    "        weight_matrix = np.zeros((self.outdim, self.outdim * self.numnet), dtype=np.float32)\n",
    "        for i in range(self.numnet):\n",
    "            q = np.array(array[i, 0].data, dtype=np.float32)\n",
    "            weight_matrix[:, i * self.outdim:(i + 1) * self.outdim] = np.identity(self.outdim, dtype=np.float32) * q\n",
    "        return Variable(weight_matrix)\n",
    "\n",
    "    \n",
    "class Generator(chainer.Chain):\n",
    "\n",
    "    def __init__(self, dim=784, num_nets=784, latent=100, wscale=0.02):\n",
    "        super(Generator, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.num_nets = num_nets\n",
    "        self.wscale = wscale\n",
    "        self.n_hidden = latent\n",
    "\n",
    "        with self.init_scope():\n",
    "            self.inter = Intersection2(self.dim, self.num_nets)\n",
    "\n",
    "            for net in range(self.num_nets):\n",
    "                w = chainer.initializers.Normal(self.wscale)\n",
    "                b = chainer.initializers.Normal(self.wscale)\n",
    "\n",
    "                setattr(self, \"l1_{}\".format(net), L.Linear(None, 48, initialW=w, initial_bias=b))\n",
    "                setattr(self, \"l2_{}\".format(net), L.Linear(None, 48, initialW=w, initial_bias=b))\n",
    "                setattr(self, \"l3_{}\".format(net), L.Linear(None, 2, initialW=w, initial_bias=b))\n",
    "\n",
    "                # set batchnormalization\n",
    "                #setattr(self, \"bn1_{}\".format(net), L.BatchNormalization(size=800))\n",
    "\n",
    "    def make_hidden(self, batchsize):\n",
    "        return np.random.uniform(-1, 1, (batchsize, self.n_hidden)).astype(np.float32)\n",
    "\n",
    "    def __call__(self, z, test=False):\n",
    "\n",
    "        for net in range(self.num_nets):\n",
    "            #h = F.relu(getattr(self, 'bn1_{}'.format(net))(getattr(self, 'l1_{}'.format(net))(z)))\n",
    "            h = F.relu(getattr(self, 'l1_{}'.format(net))(z))\n",
    "            h2 = F.relu(getattr(self, 'l2_{}'.format(net))(h))\n",
    "            # ここがreluじゃダメなの自明でしょ\n",
    "            h2 = getattr(self, 'l3_{}'.format(net))(h2)\n",
    "\n",
    "            if net == 0:\n",
    "                X = h2\n",
    "            else:\n",
    "                X = F.concat((X, h2), axis=1)\n",
    "\n",
    "        batchsize = X.shape[0]\n",
    "        X = X.reshape(batchsize, self.num_nets * self.dim)\n",
    "        x = self.inter(X.T).T\n",
    "        #x = Variable(np.reshape(x, (batchsize, 1, 28, 28)))\n",
    "        #x = Variable(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Critic(chainer.Chain):\n",
    "    def __init__(self, num_nets=784, wscale=0.02):\n",
    "        super(Critic, self).__init__()\n",
    "        self.num_nets = num_nets\n",
    "        self.wscale = wscale\n",
    "\n",
    "        with self.init_scope():\n",
    "            self.inter = Intersection2(1, self.num_nets)\n",
    "\n",
    "            for net in range(self.num_nets):\n",
    "                w = chainer.initializers.Normal(self.wscale)\n",
    "                b = chainer.initializers.Normal(self.wscale)\n",
    "\n",
    "                setattr(self, \"l1_{}\".format(net), L.Linear(None, 48, initialW=w, initial_bias=b))\n",
    "                setattr(self, \"l2_{}\".format(net), L.Linear(None, 48, initialW=w, initial_bias=b))\n",
    "                setattr(self, \"l3_{}\".format(net), L.Linear(None, 1, initialW=w, initial_bias=b))\n",
    "\n",
    "                # set batchnormalization\n",
    "                #setattr(self, \"bn1_{}\".format(net), L.BatchNormalization(size=800))\n",
    "                #setattr(self, \"bn2_{}\".format(net), L.BatchNormalization(size=800))\n",
    "\n",
    "    def __call__(self, x, test=False):\n",
    "\n",
    "        x = x.reshape(64, 2)\n",
    "        for net in range(self.num_nets):\n",
    "            # ここでhがnanになることで全てがnanになる（xは確かにnanではない）（そしてそれはその前のupdateでWがnanになってるから）\n",
    "            #h = F.leaky_relu(getattr(self, 'bn1_{}'.format(net))(getattr(self, 'l1_{}'.format(net))(x)))\n",
    "            h = F.leaky_relu(getattr(self, 'l1_{}'.format(net))(x))\n",
    "            h = F.leaky_relu(getattr(self, 'l2_{}'.format(net))(h))\n",
    "            # ここsumやめる\n",
    "            #h2 = F.sum(getattr(self, 'bn2_{}'.format(net))(getattr(self, 'l2_{}'.format(net))(h)), axis=1)\n",
    "            h2 = getattr(self, 'l3_{}'.format(net))(h)\n",
    "\n",
    "            if net == 0:\n",
    "                #Y = h2.reshape(64, 1)\n",
    "                Y = h2\n",
    "\n",
    "            else:\n",
    "                #Y = F.concat((Y, h2.reshape(64, 1)), axis=1)\n",
    "                Y = F.concat((Y, h2), axis = 1)\n",
    "\n",
    "        y = self.inter(Y.T)\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       loss/critic  loss/generator  elapsed_time\n",
      "\u001b[J1           -2.71618     1.22755         1.8684        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keiikegami/anaconda3/lib/python3.6/site-packages/ot/lp/__init__.py:211: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[J2           -4.55309     3.50314         20.0969       \n",
      "\u001b[J3           -4.29069     7.0341          36.0247       \n",
      "\u001b[J4           -4.2698      8.59974         52.2403       \n",
      "\u001b[J5           -4.19277     7.58923         68.7109       \n",
      "\u001b[J6           -4.22025     7.99906         83.975        \n",
      "\u001b[J7           -4.16781     7.9999          98.3048       \n",
      "\u001b[J8           -4.09938     7.287           112.874       \n",
      "\u001b[J9           -3.87401     7.31019         127.665       \n",
      "\u001b[J10          -3.69061     6.49659         142.918       \n",
      "\u001b[J11          -3.68058     6.13123         159.545       \n",
      "\u001b[J12          -3.60508     5.83281         174.138       \n",
      "\u001b[J13          -3.72637     4.60248         189.537       \n",
      "\u001b[J14          -3.45697     4.8139          205.147       \n",
      "\u001b[J15          -3.25043     3.89281         220.09        \n",
      "\u001b[J16          -3.16224     2.46785         235.178       \n",
      "\u001b[J17          -2.58603     4.02176         250.306       \n",
      "\u001b[J18          -2.27027     4.16576         268.107       \n",
      "\u001b[J19          -1.91128     6.39557         287.673       \n",
      "\u001b[J20          -1.85664     8.09037         305.081       \n",
      "\u001b[J21          -1.6449      8.83962         323.156       \n",
      "\u001b[J22          -1.15613     10.7971         338.543       \n",
      "\u001b[J23          -0.927805    13.4817         355.227       \n",
      "\u001b[J24          -1.16978     16.2837         370.54        \n",
      "\u001b[J25          -1.13366     18.7887         385.58        \n",
      "\u001b[J26          -0.975072    19.7            401.453       \n",
      "\u001b[J27          -0.842003    19.6663         417.679       \n",
      "\u001b[J28          -0.756392    20.7234         432.218       \n",
      "\u001b[J29          -0.852154    21.9052         446.885       \n",
      "\u001b[J30          -1.00064     22.9922         461.504       \n",
      "\u001b[J31          -0.966047    21.8812         476.423       \n",
      "\u001b[J32          -0.887741    21.9263         492.533       \n",
      "\u001b[J33          -0.797613    21.2907         508.323       \n",
      "\u001b[J34          -0.70819     20.7086         524.863       \n",
      "\u001b[J35          -0.698267    20.2232         541.212       \n",
      "\u001b[J36          -0.65728     19.0816         557.803       \n",
      "\u001b[J37          -0.600828    18.3202         573.597       \n",
      "\u001b[J38          -0.550675    17.9751         588.165       \n",
      "\u001b[J39          -0.526095    17.5971         602.809       \n",
      "\u001b[J40          -0.514438    17.6127         617.364       \n",
      "\u001b[J41          -0.524408    17.675          633.579       \n",
      "\u001b[J42          -0.445635    17.6245         649.822       \n",
      "\u001b[J43          -0.420478    17.3833         665.358       \n",
      "\u001b[J44          -0.414538    17.5164         681.63        \n",
      "\u001b[J45          -0.414996    17.2148         697.695       \n",
      "\u001b[J46          -0.437061    16.9807         715.084       \n",
      "\u001b[J47          -0.483457    16.526          732.124       \n",
      "\u001b[J48          -0.458391    15.4007         750.715       \n",
      "\u001b[J49          -0.42348     15.4381         765.495       \n",
      "\u001b[J50          -0.411169    15.3623         780.312       \n",
      "\u001b[J51          -0.480542    15.0828         796.39        \n",
      "\u001b[J52          -0.424953    14.7265         812.482       \n",
      "\u001b[J53          -0.40866     14.5336         828.232       \n",
      "\u001b[J54          -0.386426    14.3696         843.371       \n",
      "\u001b[J55          -0.401815    13.8181         858.497       \n",
      "\u001b[J56          -0.382977    13.7032         873.675       \n",
      "\u001b[J57          -0.386591    13.7477         890.852       \n",
      "\u001b[J58          -0.37398     13.7783         907.068       \n",
      "\u001b[J59          -0.383697    12.9192         922.357       \n",
      "\u001b[J60          -0.433623    12.0336         938.246       \n",
      "\u001b[J61          -0.511194    11.9051         956.186       \n",
      "\u001b[J62          -0.397129    11.9145         973.587       \n",
      "\u001b[J63          -0.406026    11.3573         991.428       \n",
      "\u001b[J64          -0.367755    11.2676         1006.63       \n",
      "\u001b[J65          -0.389668    11.0186         1021.48       \n",
      "\u001b[J66          -0.352316    10.9374         1036.43       \n",
      "\u001b[J67          -0.35798     10.6284         1051.5        \n",
      "\u001b[J68          -0.346466    10.5919         1066.82       \n",
      "\u001b[J69          -0.364066    10.3965         1082          \n",
      "\u001b[J70          -0.371097    9.9403          1097.2        \n",
      "\u001b[J71          -0.418497    9.81206         1113.25       \n",
      "\u001b[J72          -0.3547      9.86374         1129.23       \n",
      "\u001b[J73          -0.349507    9.56215         1147.13       \n",
      "\u001b[J74          -0.35171     9.69054         1162.8        \n",
      "\u001b[J75          -0.392559    9.63825         1177.77       \n",
      "\u001b[J76          -0.384517    9.10146         1192.88       \n",
      "\u001b[J77          -0.367877    9.2777          1209          \n",
      "\u001b[J78          -0.324207    8.93047         1225.58       \n",
      "\u001b[J79          -0.340988    9.0704          1242.56       \n",
      "\u001b[J80          -0.341312    9.07505         1258.32       \n",
      "\u001b[J81          -0.396975    9.13335         1274.68       \n",
      "\u001b[J82          -0.346115    9.19314         1290.27       \n",
      "\u001b[J83          -0.357985    9.05498         1306.53       \n",
      "\u001b[J84          -0.336808    8.96968         1322.6        \n",
      "\u001b[J85          -0.326252    9.1228          1340.84       \n",
      "\u001b[J86          -0.321372    9.0923          1356.95       \n",
      "\u001b[J87          -0.334765    9.0873          1372.66       \n",
      "\u001b[J88          -0.319726    9.07613         1388.28       \n",
      "\u001b[J89          -0.323872    9.13901         1404.62       \n",
      "\u001b[J90          -0.301643    8.87313         1421.02       \n",
      "\u001b[J91          -0.397248    8.79504         1437.58       \n",
      "\u001b[J92          -0.317472    8.77558         1454.84       \n",
      "\u001b[J93          -0.30258     8.48473         1472.12       \n",
      "\u001b[J94          -0.306748    8.45436         1491.99       \n",
      "\u001b[J95          -0.327108    8.49693         1510.92       \n",
      "\u001b[J96          -0.359598    8.49056         1527.32       \n",
      "\u001b[J97          -0.324058    8.02934         1543.58       \n",
      "\u001b[J98          -0.2959      8.21311         1560.29       \n",
      "\u001b[J99          -0.302157    7.98018         1578.34       \n",
      "\u001b[J100         -0.318708    8.06415         1595.15       \n",
      "\u001b[J101         -0.435868    8.19637         1611.22       \n",
      "\u001b[J102         -0.345441    7.63694         1626.8        \n",
      "\u001b[J103         -0.333141    7.31365         1642.07       \n",
      "\u001b[J104         -0.321963    7.01116         1657.99       \n",
      "\u001b[J105         -0.300905    7.14397         1673.1        \n",
      "\u001b[J106         -0.301156    7.4708          1691.21       \n",
      "\u001b[J107         -0.333926    7.45471         1710.07       \n",
      "\u001b[J108         -0.297611    7.61455         1727.19       \n",
      "\u001b[J109         -0.301516    7.73236         1742.26       \n",
      "\u001b[J110         -0.273998    7.87029         1758.51       \n",
      "\u001b[J111         -0.387101    7.61334         1774.7        \n",
      "\u001b[J112         -0.332412    7.87054         1790.54       \n",
      "\u001b[J113         -0.288964    7.61675         1806.72       \n",
      "\u001b[J114         -0.259379    7.51217         1822.67       \n",
      "\u001b[J115         -0.273093    7.43089         1838.65       \n",
      "\u001b[J116         -0.30464     7.37925         1853.88       \n",
      "\u001b[J117         -0.296709    7.60682         1869.71       \n",
      "\u001b[J118         -0.260941    7.59784         1885.76       \n",
      "\u001b[J119         -0.268432    7.55988         1901.24       \n",
      "\u001b[J120         -0.271636    7.36939         1917.48       \n",
      "\u001b[J121         -0.32628     7.28809         1935.51       \n",
      "\u001b[J122         -0.291648    7.3023          1950.74       \n",
      "\u001b[J123         -0.285412    7.06297         1965.8        \n",
      "\u001b[J124         -0.274415    7.00715         1980.8        \n",
      "\u001b[J125         -0.270069    7.11337         1995.86       \n",
      "\u001b[J126         -0.242062    7.11245         2010.91       \n",
      "\u001b[J127         -0.274821    6.99177         2025.95       \n",
      "\u001b[J128         -0.24694     6.77734         2041.06       \n",
      "\u001b[J129         -0.253952    6.68547         2056.05       \n",
      "\u001b[J130         -0.251824    6.52814         2070.95       \n",
      "\u001b[J131         -0.312214    6.43583         2086.59       \n",
      "\u001b[J132         -0.236763    6.38623         2101.6        \n",
      "\u001b[J133         -0.247298    6.08795         2116.64       \n",
      "\u001b[J134         -0.250433    6.25553         2131.7        \n",
      "\u001b[J135         -0.269833    6.33962         2146.79       \n",
      "\u001b[J136         -0.248174    6.4831          2162.3        \n",
      "\u001b[J137         -0.218182    6.33509         2177.93       \n",
      "\u001b[J138         -0.225716    6.35424         2194.23       \n",
      "\u001b[J139         -0.218992    6.28385         2210.39       \n",
      "\u001b[J140         -0.241336    6.42098         2227.07       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[J141         -0.28331     6.41442         2245.82       \n",
      "\u001b[J142         -0.26303     6.34842         2264.59       \n",
      "\u001b[J143         -0.290231    5.92525         2280.05       \n",
      "\u001b[J144         -0.295708    6.12845         2295.68       \n",
      "\u001b[J145         -0.238486    5.90295         2310.91       \n",
      "\u001b[J146         -0.247227    6.11712         2326.62       \n",
      "\u001b[J147         -0.231495    6.0464          2343.14       \n",
      "\u001b[J148         -0.252192    6.25843         2359.35       \n",
      "\u001b[J149         -0.254691    6.36387         2374.58       \n",
      "\u001b[J150         -0.230079    6.46681         2390.89       \n",
      "\u001b[J151         -0.286694    6.20153         2410.81       \n",
      "\u001b[J152         -0.263402    6.08131         2428.75       \n",
      "\u001b[J153         -0.22626     5.69941         2446.29       \n",
      "\u001b[J154         -0.231535    5.77897         2464.01       \n",
      "\u001b[J155         -0.235381    5.78658         2481.98       \n",
      "\u001b[J156         -0.226489    5.86736         2498.81       \n",
      "\u001b[J157         -0.21942     5.84837         2516.63       \n",
      "\u001b[J158         -0.208704    6.04595         2533.54       \n",
      "\u001b[J159         -0.213139    5.90061         2549.28       \n",
      "\u001b[J160         -0.244866    5.68128         2566.16       \n",
      "\u001b[J161         -0.282285    5.48566         2582.61       \n",
      "\u001b[J162         -0.225158    5.95488         2598.27       \n",
      "\u001b[J163         -0.227175    5.86684         2613.94       \n",
      "\u001b[J164         -0.235779    5.54705         2629.46       \n",
      "\u001b[J165         -0.224066    5.6868          2645.38       \n",
      "\u001b[J166         -0.211225    5.60934         2662.25       \n",
      "\u001b[J167         -0.200284    5.60318         2679.29       \n",
      "\u001b[J168         -0.185681    5.54952         2696.1        \n",
      "\u001b[J169         -0.187711    5.36542         2711.81       \n",
      "\u001b[J170         -0.190661    5.43594         2728.13       \n",
      "\u001b[J171         -0.254878    5.33834         2744.33       \n",
      "\u001b[J172         -0.202711    5.44569         2760.22       \n",
      "\u001b[J173         -0.220157    5.38219         2775.93       \n",
      "\u001b[J174         -0.161123    5.45598         2792.18       \n",
      "\u001b[J175         -0.211387    5.76268         2808.51       \n",
      "\u001b[J176         -0.21411     5.85476         2824.84       \n",
      "\u001b[J177         -0.202993    5.79985         2841.19       \n",
      "\u001b[J178         -0.194648    5.88471         2857.23       \n",
      "\u001b[J179         -0.199845    5.79128         2873.02       \n",
      "\u001b[J180         -0.21281     5.86889         2889.09       \n",
      "\u001b[J181         -0.281068    5.75363         2905.78       \n",
      "\u001b[J182         -0.229811    5.67411         2922.07       \n",
      "\u001b[J183         -0.207412    5.1977          2937.75       \n",
      "\u001b[J184         -0.21813     5.22493         2953.87       \n",
      "\u001b[J185         -0.249423    5.05195         2969.55       \n",
      "\u001b[J186         -0.215743    5.25164         2986.32       \n",
      "\u001b[J187         -0.232794    5.4524          3002.43       \n",
      "\u001b[J188         -0.180143    5.05144         3017.96       \n",
      "\u001b[J189         -0.185105    5.1465          3033.95       \n",
      "\u001b[J190         -0.181513    5.25343         3050.6        \n",
      "\u001b[J191         -0.241613    5.17817         3067.03       \n",
      "\u001b[J192         -0.194427    5.36713         3082.92       \n",
      "\u001b[J193         -0.21254     5.0227          3099.26       \n",
      "\u001b[J194         -0.191595    5.25484         3115.25       \n",
      "\u001b[J195         -0.206925    5.09698         3131.02       \n",
      "\u001b[J196         -0.212129    5.30154         3146.68       \n",
      "\u001b[J197         -0.222762    5.1328          3162.15       \n",
      "\u001b[J198         -0.202695    5.23572         3177.49       \n",
      "\u001b[J199         -0.238903    5.33146         3192.76       \n",
      "\u001b[J200         -0.22823     5.15602         3208.12       \n",
      "\u001b[J201         -0.356934    5.35725         3223.59       \n",
      "\u001b[J202         -0.234637    5.27238         3238.89       \n",
      "\u001b[J203         -0.156993    5.18247         3257.14       \n",
      "\u001b[J204         -0.174224    5.34062         3274.51       \n",
      "\u001b[J205         -0.184257    5.26495         3291.15       \n",
      "\u001b[J206         -0.162403    5.2276          3309.36       \n",
      "\u001b[J207         -0.174578    5.24627         3325.87       \n",
      "\u001b[J208         -0.194432    5.20747         3343.16       \n",
      "\u001b[J209         -0.199063    5.33929         3360.48       \n",
      "\u001b[J210         -0.193419    5.16793         3376.8        \n",
      "\u001b[J211         -0.227822    5.07771         3392.41       \n",
      "\u001b[J212         -0.184269    4.99728         3408.76       \n",
      "\u001b[J213         -0.17303     5.01903         3428.04       \n",
      "\u001b[J214         -0.173243    5.17668         3445.48       \n",
      "\u001b[J215         -0.159125    4.9352          3462.12       \n",
      "\u001b[J216         -0.161698    5.23281         3479.41       \n",
      "\u001b[J217         -0.166163    5.17672         3496.9        \n",
      "\u001b[J218         -0.181634    5.1326          3514.15       \n",
      "\u001b[J219         -0.188623    5.12989         3530.75       \n",
      "\u001b[J220         -0.195535    5.06135         3547.25       \n",
      "\u001b[J221         -0.236608    4.90758         3563.47       \n",
      "\u001b[J222         -0.164309    5.18221         3578.7        \n",
      "\u001b[J223         -0.208063    4.88619         3593.9        \n",
      "\u001b[J224         -0.207286    4.90852         3609.18       \n",
      "\u001b[J225         -0.215643    4.89037         3624.38       \n",
      "\u001b[J226         -0.175034    4.76113         3640.31       \n",
      "\u001b[J227         -0.205276    4.85371         3657.4        \n",
      "\u001b[J228         -0.207338    4.84188         3675.33       \n",
      "\u001b[J229         -0.200331    4.78515         3691.78       \n",
      "\u001b[J230         -0.177088    4.63182         3710.38       \n",
      "\u001b[J231         -0.277       4.86853         3728.43       \n",
      "\u001b[J232         -0.180471    4.57547         3744.53       \n",
      "\u001b[J233         -0.174957    4.46054         3761.96       \n",
      "\u001b[J234         -0.173129    4.61175         3778.69       \n",
      "\u001b[J235         -0.176473    4.58122         3795.92       \n",
      "\u001b[J236         -0.171134    4.52237         3812.26       \n",
      "\u001b[J237         -0.188085    4.40075         3828.44       \n",
      "\u001b[J238         -0.179673    4.25445         3846.69       \n",
      "\u001b[J239         -0.203438    4.14687         3864.71       \n",
      "\u001b[J240         -0.185095    3.41779         3881.79       \n",
      "\u001b[J241         -0.200822    3.29576         3900.91       \n",
      "\u001b[J242         -0.186537    3.35545         3919.39       \n",
      "\u001b[J243         -0.191323    3.03075         3936.38       \n",
      "\u001b[J244         -0.22131     3.3626          3953.76       \n",
      "\u001b[J245         -0.176667    3.48392         3971.41       \n",
      "\u001b[J246         -0.183868    3.25889         3989.23       \n",
      "\u001b[J247         -0.201239    3.37843         4009.62       \n",
      "\u001b[J248         -0.162326    3.47317         4027.17       \n",
      "\u001b[J249         -0.193364    3.993           4042.33       \n",
      "\u001b[J250         -0.191451    3.67744         4057.43       \n",
      "\u001b[J251         -0.21039     3.94187         4072.88       \n",
      "\u001b[J252         -0.203773    3.84926         4091.28       \n",
      "\u001b[J253         -0.196904    3.63225         4107.98       \n",
      "\u001b[J254         -0.226364    3.54364         4124.64       \n",
      "\u001b[J255         -0.252924    2.988           4141.49       \n",
      "\u001b[J256         -0.268529    3.00136         4160.2        \n",
      "\u001b[J257         -0.218907    3.18592         4178.6        \n",
      "\u001b[J258         -0.223243    2.97169         4197.01       \n",
      "\u001b[J259         -0.266848    3.33731         4215.94       \n",
      "\u001b[J260         -0.33518     3.77516         4231.95       \n",
      "\u001b[J261         -0.246412    2.55156         4247.65       \n",
      "\u001b[J262         -0.538588    2.84065         4263.75       \n",
      "\u001b[J263         -0.468136    -0.974913       4279.87       \n",
      "\u001b[J264         -0.683291    -1.80395        4296.49       \n",
      "\u001b[J265         -0.376915    -1.35242        4313.58       \n",
      "\u001b[J266         -0.707187    4.17378         4329.64       \n",
      "\u001b[J267         -1.01659     -0.415699       4346.33       \n",
      "\u001b[J268         -1.23603     -5.68673        4363.56       \n",
      "\u001b[J269         -0.597452    -5.31748        4381.19       \n",
      "\u001b[J270         -0.421173    3.10567         4400.01       \n",
      "\u001b[J271         -0.428139    -2.41506        4417.8        \n",
      "\u001b[J272         -0.457757    -1.94794        4433.59       \n",
      "\u001b[J273         -0.399583    5.21209         4448.85       \n",
      "\u001b[J274         -0.563796    -0.275633       4464.07       \n",
      "\u001b[J275         -0.66929     -5.52164        4479.23       \n",
      "\u001b[J276         -0.532621    2.31301         4494.39       \n",
      "\u001b[J277         -0.41181     3.773           4509.5        \n",
      "\u001b[J278         -0.37242     -3.96637        4527.9        \n",
      "\u001b[J279         -0.410546    0.998403        4543.96       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[J280         -0.469536    2.86157         4560.54       \n",
      "\u001b[J281         -0.490476    -3.39757        4577.82       \n",
      "\u001b[J282         -0.473256    4.14746         4593.31       \n",
      "\u001b[J283         -0.435378    -1.85615        4609.28       \n",
      "\u001b[J284         -0.507945    -1.99221        4627.03       \n",
      "\u001b[J285         -0.488461    2.83583         4644.61       \n",
      "\u001b[J286         -0.478113    -0.13022        4661.12       \n",
      "\u001b[J287         -0.454452    -4.6187         4677.08       \n",
      "\u001b[J288         -0.441078    1.30001         4692.79       \n",
      "\u001b[J289         -0.469873    3.10436         4709.41       \n",
      "\u001b[J290         -0.497143    -4.06208        4727.24       \n",
      "\u001b[J291         -0.544508    1.74446         4744.13       \n",
      "\u001b[J292         -0.522735    5.81415         4763.04       \n",
      "\u001b[J293         -0.592171    -1.30681        4780.58       \n",
      "\u001b[J294         -0.303443    1.46271         4796.76       \n",
      "\u001b[J295         -0.523459    2.54663         4813.28       \n",
      "\u001b[J296         -0.605917    7.2442          4831.05       \n",
      "\u001b[J297         -0.334156    3.26824         4848.4        \n",
      "\u001b[J298         -0.524447    10.7377         4864.16       \n",
      "\u001b[J299         -0.383335    6.98571         4879.94       \n",
      "\u001b[J300         -0.383757    7.88671         4897.39       \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
